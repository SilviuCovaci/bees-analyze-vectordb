{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seaborn\n",
    "# !pip install dask\n",
    "# !pip install dask[dataframe]\n",
    "# !pip install graphviz\n",
    "# !pip install tqdm-joblib\n",
    "# #!pip show \"dask[dataframe]\"\n",
    "#!pip install librosa\n",
    "#!pip install zarr dask numcodecs\n",
    "#!pip show numcodecs zarr\n",
    "\n",
    "#!pip install duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "from library import ExperimentConfig\n",
    "from library import GlobalVars\n",
    "import library as lib\n",
    "import data_library as data_lib\n",
    "import sqlite_library as sql_lib\n",
    "\n",
    "import helpers.dask_helper as dask_helper\n",
    "\n",
    "# use this only at the beginning\n",
    "\n",
    "\"\"\"\n",
    "STEP 1 - Synchronize metadata with available audio files.\n",
    "\n",
    "This function processes the original metadata CSV and prepares it for later use.\n",
    "It performs the following operations:\n",
    "\n",
    "1.1\n",
    "- Eliminates unnecessary columns ('lat', 'long', 'rain', 'gust speed', 'weatherID', 'time')\n",
    "- Creates a `file_name_prefix` column used to match each row with its corresponding audio files\n",
    "- Adds a column indicating whether the corresponding audio file exists on disk\n",
    "- Generates label-related columns with descriptive text (e.g., 'queen present', 'queen not present', etc.)\n",
    "- Adds extra time-based columns derived from the datetime field (e.g., day, hour)\n",
    "\n",
    "1.2\n",
    "- Creates a new, cleaned and synchronized CSV that matches existing audio files\n",
    "- Adds a `train1` column used to pre-define the train/test split\n",
    "\n",
    "Parameters:\n",
    "- `save_processed`: if True, the cleaned DataFrame will be saved to disk\n",
    "- `show_df_stats`: if True, prints summary statistics about the metadata\n",
    "\n",
    "Returns:\n",
    "- A cleaned and synchronized DataFrame (`synch_df`)\n",
    "\"\"\"\n",
    "synch_df = lib.sync_metadata_with_audio(save_processed=True, show_df_stats=False)\n",
    "print(\"Init done!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = ['mel', 'mfcc_13', 'delta_13', 'delta2_13', 'mfcc_20', 'mfcc_30', 'mfcc_40', 'mfcc_50', 'zcr', 'rms']\n",
    "#features = ['mel', 'mfcc_13', 'delta_13', 'delta2_13', 'mfcc_20', 'delta_20', 'delta2_20',  'mfcc_30', 'delta_30', 'delta2_30', 'mfcc_40', 'delta_40', 'delta2_40', 'mfcc_50', 'delta_50', 'delta2_50', 'zcr', 'rms']\n",
    "\n",
    "# \"\"\"\n",
    "# set segmentation parameters and features that will be extracted from audio files\n",
    "# features - list of features that will be extracted, ex: features = ['mel', 'mfcc_13', 'delta_13', 'delta2_13', 'mfcc_20', 'mfcc_30', 'mfcc_40', 'mfcc_50', 'zcr', 'rms']\n",
    "# \"\"\"\n",
    "features = ['pe-mfcc_40']\n",
    "GlobalVars.set_segment_lenght_and_overlap(10,5)\n",
    "\n",
    "# \"\"\"\n",
    "# STEP2\n",
    "# Each sound file will be split in more subsegments (with or without overlapping)\n",
    "# 2.1 \n",
    "# - create new csv file fith list of splitted subsegments for speficied parameters\n",
    "# \"\"\"\n",
    "df=pd.read_csv(GlobalVars.csv_data_sync_path )\n",
    "lib.generate_segmentation_metadata(df, segment_length=GlobalVars.segment_length, overlap=GlobalVars.overlap, recreate_if_exists=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# manager = multiprocessing.Manager()\n",
    "# # variable  shared between processes\n",
    "\n",
    "# global_config = manager.dict(\n",
    "#     {\"segment_lenght\": GlobalVars.segment_length,  \"overlap\": GlobalVars.overlap, \"features\": features}\n",
    "# )  \n",
    "# print(global_config)\n",
    "# \"\"\"\n",
    "# Extract defined feature for specified segmentation\n",
    "# \"\"\"\n",
    "# datasetpath = GlobalVars.extended_dataset_file_path()\n",
    "# dask_helper.process_csv(\n",
    "#     datasetpath,\n",
    "#     one_row_processor=None,\n",
    "#     partition_processor=lambda batch, partition_info: sql_lib.extract_features_from_batch(df_partition=batch, partition_info=partition_info, global_config=global_config),\n",
    "#     blocksize='500KB'\n",
    "# )\n",
    "\n",
    "# sql_lib.merge_all_partitions(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test caching for one specified configuration\n",
    "\"\"\"\n",
    "\n",
    "cache_config_dict = {\n",
    "    \"segment_lenght\": 10,            \n",
    "    \"segment_overlap\": 5,         \n",
    "    \"feature\": 'pe-mfcc_40',\n",
    "    \"vector_operation\": 'mean_iqr25'\n",
    "        \n",
    "        \n",
    "}\n",
    "cache_cfg = ExperimentConfig(cache_config_dict)\n",
    "data_lib.cache_one_configuration(cache_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create caches for all configurations \n",
    "\"\"\"\n",
    "data_lib.combine_all_data_parameters()\n",
    "data_lib.cache_all_collections()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
