# ğŸ BeeHive Acoustic Analysis with VectorDB

This repository contains the source code for the dissertation project titled  
**"Utilizarea bazelor de date vectoriale pentru monitorizarea acusticÄƒ a coloniilor de albine"**.

---

## ğŸ“¦ Clone this repository

```bash
git clone https://github.com/SilviuCovaci/bees-analyze-vectordb.git
cd bees-analyze-vectordb
```

## ğŸ“„ Project Overview

This project explores the use of acoustic analysis and vector similarity search to detect the presence of the queen bee in a beehive.

The workflow includes:
- audio preprocessing and segmentation,
- feature extraction using Mel-Frequency Cepstral Coefficients (MFCC),
- vector aggregation (mean, IQR-based, etc.),
- and classification using distance-based methods such as K-Nearest Neighbors (KNN), FAISS vector search and Milvus vector search.

All implementation details follow the methodology described in the dissertation.

## ğŸ”§ Requirements

The project requires Python 3.11 and the following key libraries:

- `numpy` â€“ numerical operations
- `librosa` â€“ audio signal processing
- `pandas` â€“ data manipulation
- `scikit-learn` â€“ machine learning models and evaluation
- `faiss-cpu` â€“ vector indexing and similarity search
- `matplotlib` or `seaborn` â€“ (optional) for visualization
- `jupyter` â€“ for running notebooks

You can install all dependencies using:

```bash
pip install -r requirements.txt
```

## ğŸš€ Usage

Follow the steps below to reproduce the workflow:

## ğŸ“¥ Download Dataset

The dataset is not included in this repository but it can be downloaded from following location:
https://www.kaggle.com/datasets/annajyang/beehive-sounds


You should manually download the audio dataset and place it in the `dataset/` directory.

> ğŸ“ Recommended structure:
> ```
> dataset/
> â””â”€â”€ soundfiles/
>     â”œâ”€â”€ 2022-06-05--17-41-01_2__segment0.wav
>     â”œâ”€â”€ 2022-06-05--17-41-01_2__segment1.wav
>     â””â”€â”€ ...
> all_data_updated.csv
> ```

## âš™ï¸ Data Preprocessing and Feature Extraction

The preprocessing pipeline involves a few key steps  
*(see Jupyter notebook: `preprocess_data.ipynb`)*:


### âœ… Step 1 â€“ Synchronize metadata with available audio files

This step prepares the initial metadata table (`all_data_updated.csv`) by:
- cleaning unused columns,
- identifying available audio files,
- assigning descriptive labels,
- creating train/test split,
- and saving a new file: `all_data_sync.csv`  with 7,100 records,

```python
synch_df = lib.sync_metadata_with_audio(save_processed=True, show_df_stats=False)
```

### âœ… Step 2 â€“ Generate segmentation metadata

This step:
- expands the synchronized metadata to reflect subsegment information based on desired segmentation parameters (e.g., 10s segments with 5s overlap).
- calculates start and end times per segment,
- generates a segmentation folder like: `extended_segments_10_sec_overlap_5`,
- in segmentation folder it creates an extended CSV for all subsegments, called `dataset_subsegments.csv`,


You can Generate segmentation metadata using:

```python
GlobalVars.set_segment_lenght_and_overlap(10,5)
df=pd.read_csv(GlobalVars.csv_data_sync_path )
lib.generate_segmentation_metadata(df, segment_length=GlobalVars.segment_length, overlap=GlobalVars.overlap, recreate_if_exists=False)
```
### âœ… Step 3 -  Feature Extraction

Feature extraction is performed on the previously segmented metadata. You can define which features to extract by editing the `features` list.

In this implementation, audio features are extracted and stored in SQLite files, one per partition. Processing is parallelized using Dask, and each partition is written independently before being merged into a single dataset, as final step.

Example:

```python
features = ['pe-mfcc_40']

manager = multiprocessing.Manager()
global_config = manager.dict({
    "segment_lenght": GlobalVars.segment_length,
    "overlap": GlobalVars.overlap,
    "features": features
})

# Define input CSV path (generated during segmentation step)
datasetpath = GlobalVars.extended_dataset_file_path()

# Run feature extraction with parallelized Dask processing
dask_helper.process_csv(
    datasetpath,
    one_row_processor=None,
    partition_processor=lambda batch, partition_info:
        sql_lib.extract_features_from_batch(
            df_partition=batch,
            partition_info=partition_info,
            global_config=global_config
        ),
    blocksize='500KB'
)

# Merge all per-partition SQLite files into a single database
data_lib.merge_all_partitions(features)
```

After processing, the output will be saved in the segmentation folder as a single .db file per feature. For example:

> ```
> extended_segments_10_sec_overlap_5/
> â”œâ”€â”€ dataset_subsegments.csv
> â”œâ”€â”€ pe-mfcc_40_features.db
> â””â”€â”€ ...
> ```

### âœ… Step 4 - Vector Aggregation & Caching

To speed up vector-based experiments, the extracted features are aggregated and saved in precomputed **cache files**.

Each cache file is defined by:
- a segmentation configuration (e.g., 10s with 5s overlap),
- one or more features (e.g., `pe-mfcc_40`),
- and an aggregation method (e.g., `mean_iqr25`, `mean`, `mean_iqr15`).

**Example: Caching a single configuration**

```python
"""
Test caching for one specified configuration
"""

cache_config_dict = {
    "segment_lenght": 10,
    "segment_overlap": 5,
    "feature": "pe-mfcc_40",
    "vector_operation": "mean_iqr25"
}

cache_cfg = ExperimentConfig(cache_config_dict)
data_lib.cache_one_configuration(cache_cfg)
```
The resulting cache file will be saved in the segmentation folder, for example:
> ```
> extended_segments_10_sec_overlap_5/
> â”œâ”€â”€ dataset_subsegments.csv
> â”œâ”€â”€ pe-mfcc_40_features.db
> â”œâ”€â”€__vectors_pe-mfcc_40_mean_iqr25.cache
> â””â”€â”€ ...
> ```
You can automatically generate and cache multiple combinations of segmentations, features, and aggregation strategies:

**Example: Caching multiple configurations**

âš ï¸ Important: Before calling cache_all_collections(), make sure that all required features
(e.g. pe-mfcc_40) have already been extracted and saved as .db files in the right segmentation folder.
Otherwise, caching will fail due to missing input data.

```python
"""
Test caching for more configurations once
"""
data_lib.combine_all_data_parameters()
data_lib.cache_all_collections()
```


Each generated .cache file contains the vector representation (after aggregation) for all subsegments and is used in later classification steps.


## ğŸ§ª Experiments and Evaluation

All experiments are grouped by method: **KNN**, **FAISS**, and **Milvus**.  
Each method has its own experiment logic and result analysis tools.

### ğŸ”¹ Structure

For each method, the following components are available:

- `*_experiment.py`: script to run the experiment on a given configuration.
- `*_playground.ipynb`: Jupyter notebook for interactive testing and parameter tuning.
- `*_results_analyze.ipynb`: notebook to load results and visualize/analyze performance metrics.

### ğŸ”„ Experiment Execution Modes

Each experiment can be executed in two ways:

- **As a standalone process**, by running the corresponding Python script with a set of parameters (e.g., via CLI).
- **As a library call**, by importing and invoking the experiment function directly from another script or notebook.

When running multiple experiments in parallel (e.g., different configurations or datasets), each experiment is executed as an **independent process**, allowing parallel execution across multiple CPU cores.

This hybrid design offers both flexibility (for interactive testing) and scalability (for batch execution).


### ğŸ”§ KNN - Experiment Preparation

Before running KNN experiments, a set of configuration combinations is generated.  
These include variations of parameters such as distance metric, number of neighbors, feature aggregation method, and segmentation settings.

The list of experiments is saved to a CSV file (e.g., `all_knn_experiments_v3.csv`), and the results of executed experiments will be appended to another file.

If needed, you can customize which combinations are generated by modifying the logic inside the `combine_knn_multiple_v3()` function from `knn_experiment.py`.

Example:

```python
all_knn_experiments_file_path = GlobalVars.experiments_path + "all_knn_experiments_v3.csv"
knn_experiments_output_file = GlobalVars.experiments_path + "executed_knn_configs_results_v3.csv"

# Generate all experiment combinations
knn_tool.combine_knn_multiple_v3(all_knn_experiments_file_path)
```

### â–¶ï¸ KNN â€“ Running a Single Experiment

You can run a single experiment configuration in two ways: as an individual **process** or as a direct **library function call**.

---

#### ğŸ”¹ Option 1 â€“ Run as a separate process

```python
# Execute ONE configuration by launching it as an independent process
cfg_records = pd.read_csv(all_knn_experiments_file_path)
row = cfg_records.iloc[0].copy()

new_row = executor.launch_execute_configurations_as_process(
    knn_tool,
    row,
)
```
The result represents a row from the results CSV file and includes evaluation metrics such as accuracy and per-class precision, as well as performance measurements for both the training and prediction steps.

#### ğŸ”¹ Option 2 â€“ Run directly via library call

```python
# Execute ONE configuration by calling the experiment function directly
cfg_records = pd.read_csv(all_knn_experiments_file_path)
row = cfg_records.iloc[0].copy()
cfg = ExperimentConfig(row)
GlobalVars.set_segment_lenght_and_overlap(cfg._SEGMENT_LENGHT, cfg._SEGMENT_OVERLAP)
knn_results = knn_tool.execute_configuration(cfg)
```
The knn_results is a dictionary that includes detailed evaluation metrics, such as the confusion matrix and a full classification report.

### ğŸ” Running All KNN Experiments (with Repeats)

You can run all experiment configurations from the CSV file in multiple rounds, e.g. for statistical analysis or robustness testing.

```python
# Execute all configurations from the generated experiment file
# Repeating the full experiment N times

repeat_no = 5

for i in range(repeat_no):
    print(f"Execute round {i}")

    knn_experiments_output_file = GlobalVars.experiments_path + \
        f"executed_knn_configs_results_v3_round{i}.csv"

    executor.process_all_configs_by_threads(
        exec_tool=knn_tool,
        input_file_path=all_knn_experiments_file_path,
        output_file_path=knn_experiments_output_file,
        num_threads=4,
        chunk_size=20
    )
```
Each round will process all configurations in parallel (using threads) and save the results to a separate output file (one per round).

> ğŸ”„ **Note:** To extend the dataset across all segmentations of the same duration, you can use the special segmentation format `(10, all)`.  
> This will combine all available segmentations with a length of 10 seconds, regardless of overlap, into a single dataset. This is useful for increasing sample diversity and improving generalization.


### âš¡ FAISS Experiments

FAISS is used for efficient vector similarity search using index structures such as `Flat`, `IVF`, `PQ`, and `HNSW`.

Compared to KNN, FAISS experiments are more complex, as they involve additional indexing-specific parameters.  
Each experiment combines dataset-related parameters (features, segmentations, aggregations) with index-specific configurations (e.g., `nlist`, `nprobe`, `M`, `nbits`).

---

### ğŸ”§ Preparing FAISS experiment configurations

Experiments are generated **per index type**. For each type (e.g., `ivf`, `flat`, `hnsw`), a predefined list of index parameters is available. These can be extended or customized in the code.

Example: generating all experiments for IVF indexes.

```python
index_type = "ivf_all"

all_faiss_experiments_file_path = GlobalVars.experiments_path + \
    f"all_faiss_experiments_index_{index_type}.csv"

lib.combine_multiple_parameters_v3(
    all_faiss_experiments_file_path,
    getattr(ex_cfg, index_type)  # predefined list of IVF-specific index configurations
)
```
ğŸ› ï¸ You can customize or extend the list of index parameters in the ex_cfg.<index_type> dictionary.

### â–¶ï¸ FAISS â€“ Running Experiments

FAISS experiments can be executed in a similar way to KNN, using three approaches:

1. **Single configuration as a separate process**  
2. **Single configuration as a direct function call**  
3. **Multiple configurations in parallel**, by reading from a CSV file

---

#### ğŸ”¹ 1. Run one configuration as a separate process

```python
# Execute ONE configuration by launching it as an independent process
cfg_records=pd.read_csv(all_faiss_experiments_file_path)
row = cfg_records.iloc[0].copy()
new_row = executor.launch_execute_configurations_as_process(
    faiss_tool,
    row,
)
```
The result represents a row from the results CSV file and includes evaluation metrics such as accuracy and per-class precision, as well as performance measurements for both the training and prediction steps.

#### ğŸ”¹ 2. Run one configuration via function call

```python
# Execute ONE configuration by calling the experiment function directly
cfg_records=pd.read_csv(all_faiss_experiments_file_path)
row = cfg_records.iloc[0].copy()
cfg = ExperimentConfig(row)
GlobalVars.set_segment_lenght_and_overlap(cfg._SEGMENT_LENGHT, cfg._SEGMENT_OVERLAP)
faiss_results = faiss_tool.execute_configuration(cfg)
```

The `faiss_results` is a dictionary that includes detailed evaluation metrics, such as the confusion matrix and a full classification report.

### ğŸ” Running All FAISS Experiments (with Repeats)

You can run all experiment configurations from the CSV file in multiple rounds, e.g. for statistical analysis or robustness testing.

```python
# Execute all configurations from the generated experiment file
# Repeating the full experiment N times

repeat_no = 5
index_type = 'ivf_all'

all_faiss_experiments_file_path = GlobalVars.experiments_path + f"all_faiss_experiments_index_{index_type}.csv"
lib.combine_multiple_parameters_v3(all_faiss_experiments_file_path, ex_cfg.ivf_all)

for i in range(repeat_no):
    print(f"Execute round {i}")

    #you can cache the indexes before prediction
    #cache_all_faiss_indexes(all_faiss_experiments_file_path)
    
    faiss_experiments_output_file = GlobalVars.experiments_path + f"executed_faiss_experiments_index_{index_type}_round{i}.csv"
    executor.process_all_configs_by_threads(exec_tool=faiss_tool, input_file_path=all_faiss_experiments_file_path, output_file_path=faiss_experiments_output_file, num_threads = 4, chunk_size = 20)
```
Each round will process all configurations in parallel (using threads) and save the results to a separate output file (one per round).

> ğŸ”„ **Note:** To extend the dataset across all segmentations of the same duration, you can use the special segmentation format `(10, all)`.  
> This will combine all available segmentations with a length of 10 seconds, regardless of overlap, into a single dataset. This is useful for increasing sample diversity and improving generalization.


### âš¡  Milvus Experiments

Milvus is a powerful open-source vector database optimized for high-performance similarity search at scale.  
In this project, Milvus is used to evaluate vector-based classification and retrieval using a real vector database server environment.

Unlike KNN or FAISS (which run entirely in Python or C++), Milvus runs as a **separate service**, typically inside a Docker container.  
Experiments communicate with Milvus via the official Python API (`pymilvus`), using a custom helper module that abstracts collection creation, data insertion, and query execution.

---

### ğŸš€ Milvus Setup

To run Milvus experiments, you need to install and start Milvus locally using Docker.  
We recommend using the **standalone version**, which includes both the server and the metadata store in a single container.

ğŸ“„ Installation guide (official):  
â¡ï¸ [Milvus Standalone â€“ Windows (Docker)](https://milvus.io/docs/install_standalone-windows.md)

> ğŸ’¡ For Linux/macOS, see the dropdown menu on the same page for platform-specific instructions.

Once Milvus is up and running, experiments can connect to the local server (`localhost:19530`) using the Python client.

---

### ğŸ§© Milvus Integration in this Project

- Collection creation is done before running the experiments, based on feature configuration and vector schema.
- Each experiment inserts vectors into a new or existing Milvus collection and performs similarity search queries.
- The project includes a custom `milvus_helper.py` module to simplify all interactions (connect, create, insert, query, delete).

The rest of the experimentation flow (configuration handling, metrics, batch execution) follows a structure similar to KNN and FAISS.

### ğŸ”§ Preparing Milvus Experiment Configurations

Milvus requires a slightly different preparation flow compared to KNN and FAISS. Since Milvus is a standalone vector database running in a Docker container, all data must first be inserted into **collections**, which are indexed and queried via API.

#### ğŸ”¹ 1. Generate configurations to be tested

In this project, we focused on the IVF index type and defined multiple IVF-specific configurations to be tested. 

Example: generating all Milvus experiments using predefined IVF configurations.

```python
index_type="milvus_pq_10"
all_milvus_experiments_file_path = GlobalVars.experiments_path + \
    f"all_milvus_experiments_index_{index_type}.csv"
lib.combine_multiple_parameters_v3(
    all_milvus_experiments_file_path, 
    getattr(ex_cfg, index_type) # predefined list of IVF-specific Milvus index configurations
)
```

#### ğŸ”¹ 2. Define the collection(s)

#### ğŸ”¹ 2. Define the collection(s)

In the approach used in this project, a separate Milvus collection is created for each unique combination of:

- segmentation (`segment_lenght`, `segment_overlap`)
- extracted feature (e.g., `pe-mfcc_40`)
- aggregation method (e.g., `mean`, `mean_iqr25`)

The collection name is generated based on these parameters.  For example: `pe_mfcc_40_vectors_mean_iqr25_len10_overlapall`.

Collections can be created either individually or iteratively (in batch), prior to running the actual experiments.  This ensures that all data is preloaded and indexed before evaluation begins.

The following example demonstrates how to define a configuration and populate the corresponding Milvus collection:

```python
def build_configuration_all10sec():
    cfg_dict = {
        "segment_lenght": 10,
        "segment_overlap": "all",
        "feature": "pe-mfcc_40",
        "vector_operation": "mean",
        "metric_type": "COSINE",
        "index_params": {"index_type": "IVF_FLAT"},
        "normalize": 1,
        "vote_type": "uniform",
        "neighbors": 15,
    }

    cfg = ExperimentConfig(cfg_dict)
    return milvus_tool.create_and_fill_collection_for_specified_configuration(cfg, True)

# Create and populate the collection
build_configuration_all10sec()
```
The collection will be indexed using the default IVF_FLAT index, and populated with vectors extracted for that configuration.

### â–¶ï¸ Running Milvus Experiments

Unlike KNN and FAISS, Milvus experiments are currently executed only via **library function calls** â€” no separate process-based execution was implemented in this version.

To run a single experiment from a configuration row:

```python
cfg = ExperimentConfig(row)
GlobalVars.set_segment_lenght_and_overlap(cfg._SEGMENT_LENGHT, cfg._SEGMENT_OVERLAP)

metrics_json = milvus_tool.execute_configuration(cfg)
new_row = lib.extract_experiment_results(
    new_row=row,
    tool_results=metrics_json,
    process_key=f"row_{index}",
    error=""
)
```
For full batch execution of all experiment defined in the previously generated configuration file, see the notebook: `milvus_playground.ipynb`.

### ğŸ“Š Results & Analysis

Each experiment outputs a result row that includes classification metrics (e.g., accuracy, per-class precision)  
as well as performance indicators (training/prediction time, indexing statistics if applicable).

Results are saved to CSV files for each method:
- `executed_knn_configs_results_*.csv`
- `executed_faiss_experiments_index_*.csv`
- `executed_milvus_experiments_index_*.csv`

There are dedicated Jupyter notebooks for result analysis:
- `knn_results_analyze.ipynb`
- `faiss_results_analyze.ipynb` *(can also be used to analyze Milvus results)*

Each notebook includes:
- filtering by configuration parameters,
- grouping and sorting by accuracy or other metrics,
- visualizations (confusion matrix, accuracy distribution, pie charts, bar plots, etc.)

> ğŸ” These tools help identify the best-performing configurations and understand the impact of various parameters.






